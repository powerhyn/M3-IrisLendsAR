# IrisLensSDK 의사결정 기록

## 1. 프로젝트 목표

### 1.1 핵심 기능
- 실시간 카메라 영상에서 사람의 홍채를 추적
- 홍채 위에 가상 렌즈(콘택트렌즈) 오버레이
- SDK/라이브러리 형태로 Flutter, Android, iOS, Web에서 사용 가능하게 제공

### 1.2 핵심 요구사항
- 실시간 처리 (30fps 이상)
- 크로스플랫폼 지원
- 외부 앱에서 SDK로 호출 가능
- 추후 모델 교체 가능한 구조

---

## 2. 홍채 검출 기술 선택

### 2.1 선택지 분석

#### Option A: MediaPipe
| 항목 | 내용 |
|------|------|
| **장점** | - Google이 대규모 데이터로 학습 완료<br>- 468개 얼굴 랜드마크 + 10개 홍채 포인트 제공<br>- 모바일 최적화됨 (30fps+ 가능)<br>- Apache 2.0 라이선스 (상용 무료)<br>- Android/iOS/Web 공식 SDK 존재 |
| **단점** | - 얼굴 전체가 인식되어야 홍채 검출 가능<br>- 눈만 클로즈업 시 실패<br>- 커스터마이징 제한적 |
| **개발 기간** | 수일~수주 |

#### Option B: OpenCV + 직접 학습
| 항목 | 내용 |
|------|------|
| **장점** | - 완전한 커스터마이징 가능<br>- 특수 상황 대응 가능<br>- 자체 IP/특허 확보 가능 |
| **단점** | - 학습 데이터 필요 (수만~수십만 장)<br>- 라벨링 작업 비용/시간 큼<br>- 모바일 최적화 직접 수행 필요<br>- 동일 수준 도달에 수개월 소요 |
| **개발 기간** | 3~6개월 (경험 있는 ML 엔지니어 기준) |

#### Option C: 하이브리드 (MediaPipe + 커스텀)
| 항목 | 내용 |
|------|------|
| **장점** | - MediaPipe의 장점 활용<br>- 한계 상황을 커스텀 모델로 보완<br>- 점진적 개선 가능 |
| **단점** | - 두 시스템 유지보수 필요<br>- 전환 로직 복잡성 |
| **개발 기간** | Phase 1: 수주 + Phase 2: 수주 |

### 2.2 결정: Option C (하이브리드) - Phase 분리 접근

**선택 이유:**
1. MVP를 빠르게 출시하여 시장 검증 가능
2. MediaPipe의 한계가 실제로 문제인지 데이터 수집 후 판단
3. 추후 Eye-Only 모델 추가로 한계 보완 가능
4. 인터페이스 추상화로 모델 교체 용이

---

## 3. MediaPipe 한계점 분석

### 3.1 구조적 의존성

```
MediaPipe 파이프라인:
얼굴 전체 인식 (필수) → Face Mesh 생성 → 홍채 랜드마크 추출

문제: 얼굴 인식 실패 시 전체 파이프라인 실패
```

### 3.2 실패 상황 목록

| 상황 | MediaPipe 동작 | 렌즈 피팅 앱에서의 영향 |
|------|----------------|----------------------|
| 눈만 클로즈업 | ❌ 실패 | 사용자가 거울보듯 눈 확대 시 동작 안함 |
| 얼굴 반쪽만 | △ 불안정 | 한쪽 눈만 보려할 때 불안정 |
| 극단적 각도 (45°+) | △ 불안정 | 다양한 각도 확인 시 품질 저하 |
| 선글라스/두꺼운 안경테 | △ 불안정 | 안경 착용자 사용 어려움 |
| 역광 | △ 불안정 | 조명 환경에 민감 |

### 3.3 렌즈 피팅 앱 특성상 문제되는 이유

**사용자 기대 행동:**
- 거울 보듯이 눈 가까이 확대해서 보기
- 한쪽 눈만 크게 보기
- 다양한 각도에서 렌즈 확인

→ 이 행동들이 MediaPipe의 약점과 정확히 일치

### 3.4 해결 전략

**Phase 1 (MVP):**
- MediaPipe 사용
- UX 가이드로 "얼굴 전체가 보이게" 유도
- 실패 케이스 데이터 수집

**Phase 2 (개선):**
- Eye-Only 모델 학습 또는 오픈소스 활용
- 하이브리드 시스템 구축 (MediaPipe 실패 시 Eye-Only로 전환)

---

## 4. 코어 엔진 언어 선택

### 4.1 선택지 분석

#### Option A: C++ 코어 + 플랫폼별 바인딩
| 항목 | 내용 |
|------|------|
| **장점** | - 최고 성능 (실시간 처리에 중요)<br>- 코어 로직 1회 작성으로 전체 플랫폼 지원<br>- MediaPipe, OpenCV가 C++로 작성됨<br>- 모든 플랫폼에서 네이티브 성능 |
| **단점** | - 초기 설정 복잡<br>- 플랫폼별 바인딩 작업 필요<br>- 빌드 환경 구성 난이도 |

#### Option B: Flutter 중심 (Dart + FFI)
| 항목 | 내용 |
|------|------|
| **장점** | - Flutter가 메인이라면 빠른 개발<br>- google_mlkit 플러그인 활용 가능 |
| **단점** | - Native Android/iOS 앱에서 사용 불가<br>- Web 지원 제한적<br>- SDK 형태 배포 어려움 |

#### Option C: 각 플랫폼별 네이티브
| 항목 | 내용 |
|------|------|
| **장점** | - 플랫폼 최적화 가능 |
| **단점** | - 코드 중복 (3벌 이상)<br>- 유지보수 비용 높음<br>- 기능 일관성 보장 어려움 |

### 4.2 결정: Option A (C++ 코어)

**선택 이유:**
1. 성능이 가장 중요 (실시간 30fps 이상 필요)
2. MediaPipe, OpenCV가 C++로 작성되어 자연스러운 통합
3. 코어 로직 한 번 작성으로 모든 플랫폼 지원
4. SDK 형태 배포에 가장 적합

---

## 5. SDK 배포 형태

### 5.1 플랫폼별 빌드 결과물

| 플랫폼 | 바인딩 기술 | 빌드 결과물 | 배포 방식 |
|--------|-------------|-------------|----------|
| Android | JNI | libiris_sdk.so → AAR | Maven Central |
| iOS | Objective-C++ | IrisSDK.xcframework | CocoaPods |
| Flutter | dart:ffi | Flutter Plugin | pub.dev |
| Web | Emscripten | WASM + JS | npm |

### 5.2 C API 래퍼가 필요한 이유

```
C++ Core
    │
    ▼ (C API로 래핑)
extern "C" 함수들
    │
    ├── JNI에서 호출 가능
    ├── Objective-C++에서 호출 가능
    ├── dart:ffi에서 호출 가능
    └── WASM에서 호출 가능
```

C++의 name mangling 문제를 피하고, 모든 바인딩에서 일관되게 호출 가능

---

## 6. 추후 모델 교체 가능성

### 6.1 설계 원칙: 인터페이스 추상화

```
IrisDetector (추상 인터페이스)
    ├── MediaPipeDetector (Phase 1)
    ├── EyeOnlyDetector (Phase 2)
    └── HybridDetector (Phase 2)
```

### 6.2 교체 가능한 부분

| 부분 | 교체 난이도 | 설명 |
|------|-------------|------|
| 홍채 모델만 | ⭐⭐ 중간 | 가장 현실적, MediaPipe 구조 유지 |
| Face Mesh + 홍채 | ⭐⭐⭐ 어려움 | 더 많은 작업 필요 |
| 전체 파이프라인 | ⭐⭐⭐⭐ 매우 어려움 | 사실상 새로 만드는 것 |

### 6.3 커스텀 모델 요구사항

**출력 형식 호환:**
- MediaPipe 홍채 출력: 10개 포인트 (x, y)
- 커스텀 모델도 같은 형식 출력 시 렌더링 코드 재사용 가능

**변환 경로:**
- PyTorch → ONNX → TFLite
- TensorFlow → TFLite (직접)

---

## 7. Eye-Only 모델 옵션 (Phase 2)

### 7.1 오픈소스 후보

| 프로젝트 | 특징 | 장단점 |
|----------|------|--------|
| **dlib eye detector** | 가볍고 빠름 | 정확도 중간 |
| **OpenCV Haar Cascade** | 매우 가벼움 | 정확도 낮음 |
| **RT-GENE** | 눈 영역 특화 | 학술용, 상용 확인 필요 |
| **GazeML** | 시선 추적용 | 홍채 검출 포함 |

### 7.2 직접 학습 시 필요사항

| 항목 | 요구량 |
|------|--------|
| 학습 데이터 | 5,000~10,000장 (눈 영역 크롭) |
| 라벨링 | 홍채 경계 마킹 |
| 아키텍처 | U-Net, HRNet, MobileNet 기반 |
| 학습 인프라 | GPU 서버 |
| 최적화 | 양자화 (INT8), pruning |

---

## 8. 개발 우선순위

### 8.1 Phase 1 순서

```
1. 환경 설정 (Week 1-2)
   └── MediaPipe + OpenCV 빌드

2. 코어 엔진 (Week 3-4)
   └── IrisDetector 인터페이스 + MediaPipe 구현 + 렌즈 렌더링

3. Android 바인딩 (Week 5-6)
   └── JNI + 데모 앱 (가장 테스트 쉬움)

4. iOS/Flutter 바인딩 (Week 7-8)
   └── 추가 플랫폼 확장
```

### 8.2 Android 우선 선택 이유

1. 실기기 테스트가 가장 쉬움 (USB 연결만으로 가능)
2. 빌드 환경 설정이 상대적으로 단순
3. 디버깅 도구가 풍부 (Logcat, Profiler)
4. 에뮬레이터 카메라 시뮬레이션 가능

---

## 9. 리스크 분석

| 리스크 | 확률 | 영향 | 대응 방안 |
|--------|------|------|----------|
| MediaPipe 빌드 복잡성 | 높음 | 일정 지연 | 사전 빌드된 바이너리 활용 검토 |
| 성능 미달 (30fps 미만) | 중간 | 품질 저하 | GPU 가속 우선 적용, 해상도 조절 |
| 모바일 메모리 제한 | 중간 | 크래시 | 모델 경량화, 메모리 풀링 |
| MediaPipe 한계가 심각 | 중간 | 기능 제한 | Phase 2 Eye-Only 모델 조기 착수 |
| Eye-Only 모델 정확도 부족 | 중간 | 기능 제한 | 데이터 추가 수집, 앙상블 |

---

## 10. 라이선스 검토

| 라이브러리 | 라이선스 | 상용 사용 | 소스 공개 의무 |
|------------|----------|----------|---------------|
| MediaPipe | Apache 2.0 | ✅ 가능 | ❌ 없음 |
| OpenCV | Apache 2.0 | ✅ 가능 | ❌ 없음 |
| TensorFlow Lite | Apache 2.0 | ✅ 가능 | ❌ 없음 |
| dlib | Boost | ✅ 가능 | ❌ 없음 |

→ 모든 주요 의존성이 상용 친화적 라이선스

---

## 11. 결론 요약

| 결정 항목 | 선택 | 핵심 이유 |
|----------|------|----------|
| 홍채 검출 | MediaPipe (Phase 1) → 하이브리드 (Phase 2) | 빠른 MVP + 점진적 개선 |
| 코어 언어 | C++ | 성능 + 크로스플랫폼 |
| 빌드 시스템 | CMake | 크로스플랫폼 빌드 |
| 첫 타겟 플랫폼 | Android | 테스트 용이성 |
| 아키텍처 | 인터페이스 추상화 | 모델 교체 유연성 |
